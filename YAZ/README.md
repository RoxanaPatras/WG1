# Tabulating imprints

This folder contains scripts for tabulating the number and dates of
published copies of titles from an ELTeC language collection. The
procedure used is to generate a file of queries to be submitted for processing
by a Z3950 server, which should then return a collection of MARC 
records. These MARC records are in turn converted into a simple XML format,
from which a summary of the imprint data is generated by another stylesheet.

The assumption behind this approach is primarily that a search
against the relevant national library will provide more
focussed and less noisy results than a search against a union
catalogue such as WorldCat. Using a standard 
interface such as Z3950 and a standard format such as MARC is also (probably) 
a more reliable method of getting good quality data than screen scraping.

These assumptions have to be put to the test, however.

## Installation and deployment
To run these scripts, in addition to the scripts themselves you
need the following software installed:

- saxon
- yaz-client

YAZ (which is presumably an acronym for Yet Another Z3950-client) is
readily available for Windows, Linux, or Mac. On Linux you can install
it by typing e.g. `sudo apt install yaz`

Here's how to run the scripts:
 
### Generate the query strings 
     
```
saxon -xi *driver.tei* querygenerator.xsl > query.txt
```

`driver.tei` is the driver file for the language collection concerned, which
is available at the root of each collection: for example `ELTeC-FRA/driver.tei`
    
The query strings are saved in the file `query.txt`. It is strongly recommended
that this should be checked carefully for plausibility. The querygenerator
does its best to simplify the titles and authors in the TEI headers
so as to generate good results, but this is not an entirely transparent process.

### Run the query

The exact mechanics of this will depend on the Z3950 server targetted. Any
such server is required to provide documentation about (for example)
any access constraints, the formats in which results are delivered, 
the forms of query supported etc. Every server I have so far
encountered supports the query language used by the querygenerator stylesheet;
some of them however generate records in USMARC while others
use UNIMARC. 

Some experimentation may therefore be necessary, which is why  I find it 
convenient to save details of the login requirements in a
shell script such as `goBNF`. This can be run to start a yaz client
interactive session within which the query itself  can be tested
interactively. Here for example is an interactive session
accessing the Bibliotheque Nationale's Z3950 server:
```
Z> find @and @attr 1=1003 ponson @attr 1=4 "la baronne trépassée "   
Sent searchRequest.
Received SearchResponse.
Search was a success.
Number of hits: 11, setno 2
records returned: 0
Elapsed: 0.212988
Z> 
```
This command has find 11 records where the author field contains "ponson"
and the title field contains "la baronne trepassee". To see them, 
we use the `show` command. By default, yaz-client will try to show
records in US Marc format, which is not available on this particular
server. We must therefore start our session with a `format` command
requesting that records be displayed in Unimarc instead.

```
Z> format unimarc
Z> show all
```
If we have already prepared all the queries we want to run in a
file, called (say) FRAquery.txt, we could invoke that file from the

yaz command line:
```
Z>. FRAquery.txt
```
It;s probably easier however, 
once the right incantations have been determined, 
to run the whole process  at the command
line using `yaz-client` like this

```
mkdir FRArecords
cd FRArecords
yaz-client -u Z3950/Z3950_BNF -f ../FRAquery.txt z3950.bnf.fr:2211/TOUT-UTF8
cd ..
```

The `marcdump` commands in the query file save a set of marc records
for each title in a file named for the XML id of the text concerned
with the suffix `.usmarc`. It's convenient to save
these in a separate folder, called `FRArecords` in the example above.

### Convert MARC records to XML

Once upon a time, the Library of Congress invented an XML format
for MARC called marcxml. This was not the world's greatest success. Other
XML formats (notably MODES) seem more popular in the library
community. So I have invented my own breathtakingly naive XML
format to make it possible to process the MARC dump records
into something anyone can understand with tools that I understand. The
script is in Perl and it's called `xmlify.prl` : you could
run it over all the records generated in the previous step
like this

```
for f in FRArecords/*.usmarc; do echo $f \
   perl xmlify.prl $f > $.xml; done
```

### Extract useful data from the XMLified records    
  
At last we can do something useful with our records. The
XSLT stylesheet `selectImprints.xsl` for example looks
for imprint data in a Unimarc file, and generates a minimal TEI bibl
record sorted for each distinct one found. 

We can run the script across the whole of the records we generated
in the previous step like this:

```
     echo "<listBibl>" > fr_imprints.xml; 
     for f in FR*.xml; do echo $f; \
         saxon docId=$f $f ../selectImprints.xsl >>fr_imprints.xml; done
    cat "<\/listBibl> >> fr_imprints.xml

```
And yes, of course, we could combine the last two steps into
one or write a Python script to run each step one at a time, or...
but that is left as an exercise for the reader.
