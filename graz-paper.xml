<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <title>In search of comity: TEI for distant reading</title>
            <author xml:id="LB42">Lou Burnard</author>
            <author>Christof Sch√∂ch</author>
            <author>Carolin Odebrecht</author>
         </titleStmt>
         <publicationStmt>
            <p>Unpublished draft</p>
         </publicationStmt>
         <sourceDesc>
            <p>Original source</p>
            <!-- <p> I very much like the way the paper explains all the various aspects of the things
               that WG1 has been doing and how ELTeC is being built. And I see some "anchors" in
               there, that is some strong or particular ideas, like the idea of "comity", the idea
               of the "ODD chaining" or the idea of ELTeC supporting distant reading methods. </p>
            <p> What I think would make the paper stronger is if these strong ideas would be
               connected to each other in a more explicit way. Like saying: what is the contribution
               of ODD chaining to comity? Or how does ODD chaining help us best support distant
               reading methods? Is ODD chaining in any way related to the idea of "what is text,
               really?"? The most original part of the paper is probably that of comity, but then
               the paper could make an even stronger case of how ELTeC in a way recovers a comity in
               TEI that had been there from the start but was somehow forgotten on the way for a
               while? </p>
            <p> Also (and possibly related to the last point), I think the paper would benefit from
               a more obvious (or more explicit) way of structuring the information provided about
               ELTeC: either narrative (the story of how we arrived at this strategy; I don't
               recommend) or in some idealized description of the process (also tricky), or
               following some principle of increased complexity / innovation (from plain text to
               ODD-chaining?). Or: how does it all support comity? </p>
            <p> A detail: I don't think level-2 is adequately described as the layer in which the
               results of the Distant Reading methods are stored. Rather, some annotation levels
               based on linguistic annotation methods (Tokenization, Lemmatization, POS-Tagging,
               Named Entity Recognition) are available there, and these annotations are relevant to
               or useful for Distant Reading methods, such as stylometry, topic modeling, etc. </p>
            <p> The cross-European approach of ELTeC could be added as a argument for comity: not
               just across disciplines and approaches, but also across languages / countries. After
               all, this is one of the few true originalities of ELTeC so should be
               foregrounded.</p>-->
         </sourceDesc>
         <!-- comments from reviewers
      
> Review 1
> ========
> This submission describes work done to create a corpus of TEI data
> specifically aimed at supporting text analysis and possibly enhancing its
> results.
>
>
> Quality of Content     (20%): 8
> Thematic Relevance     (20%): 10
> Overall Recommendation (60%): 8
> Total points (out of 100)   : 84
>
>
> 
> This project is important and valuable, and the proposal offers a clear and
> detailed view that will be useful for conference participants. The use of
> ODD-chaining is of particular interest. If space had permitted, I would have
> liked to see some comparison with previous efforts in this direction (I am
> thinking in particular of the TEI-Analytics schema developed for the MONK
> project some years ago), and I would also have liked to see some additional
> information about the specific kinds of markup that most effectively support a
> richer and better-informed distant reading. I hope that if accepted this paper
> will cover those details more fully.
>
> The question of "what text really is" is provocative here, particularly the
> idea that fields like authorship attribution, topic modeling, etc. have a
> theory of  text different from the one espoused by the those who work on text
> markup. Again, I understand that the brevity of the proposal format doesn't
> make it easy to explore this idea in detail here, but I think this is a really
> interesting and important question that has consequences for corpus
> design/construction, so I hope it will be discussed at greater length in the
> finished paper.
> -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
>
>
> Review 2
> ========
>
 The paper reports on developments in the context of the COST Action CA16204
> "Distant Reading", in particular TEI-conformant schemas developed for the
> European Literary Text Collection (ELTeC).
>
>> Quality of Content     (20%): 8
> Thematic Relevance     (20%): 8
> Overall Recommendation (60%): 10
> Total points (out of 100)   : 92
>
>
> 
> The submission discusses the "Distant Reading" COST action and the European
> Literary Text Collection (ELTeC).
>
> Both are focussing on issues which are highly significant and relevant to the
> TEI community (multilingualism, metadata semantization, textual analytics).
> The paper proposes to elaborate on the question posed in the conference topic
> from the perspective of statistically-derived authorship attribution, topic
> modelling and stylistic analysis.
>
> Furthermore, the paper will demonstrate an ODD (or chain of ODDs) aimed at
> distant reading rather than representation of the original texts, which is
> perfectly aligned with the topic of the conference and touches on a topic
> (broadly speaking "TEI and Distant Reading) that is - if the contributions to
> the current conference are considered - gaining prominence and recognition.
>
> The abstract is concise and clear, and a number of arguments for decisions
> taken are already previewed in it. It will be very interesting to hear more
> about the actual implementation and the specific challenges encountered,
> particularly regarding multilingualism.
>
> -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
>
>
> Review 3
> ========
>
>> The paper introduces the ELTeC specific TEI customisation preparing a large
> corpus of novels for distant reading purposes.
>
>
> Evaluation of the Contribution
>> Quality of Content     (20%): 10
> Thematic Relevance     (20%): 6
> Overall Recommendation (60%): 9
> Total points (out of 100)   : 86
>
>
> The ELTeC is certainly an important corpus in creation and the use of the TEI
> for its encoding is an important step for the dissemination of the
> capabilities of the TEI in the stylometric community. The proposal creates
> only a thin line to the main theme of the conference and could make more clear
> how the encoding guidelines for the corpus help to understand better "what
> text really is", or - my suggestion - just avoid unnecessary buzzwording. A
> discussion how the encoding decisions are related to the task the corpus
> should serve, is interesting in itself.
>
>


>
> -->
      </fileDesc>
      <profileDesc>
         <textClass>
            <keywords>
               <term>distant reading</term>
               <term>ELTeC</term>
               <term>ODD chaining</term>
               <term>corpus design</term>
               <term>the european novel</term>
               <term>literary studies</term>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <change when="2019-07-15" who="#LB42">working through CO's comments</change>
         <change when="2019-07-08" who="#LB42">circulated first draft to CO and CS</change>
         <change when="2019-07-01" who="#LB42">started first draft</change>
         <change when="2019-04-24" who="#LB42">finalised abstract</change>
      </revisionDesc>
   </teiHeader>
   <text>
      <front>
         <div type="abstract">
            <head>Abstract</head>
            <p>Any expansion of the TEI beyond its traditional user-base involves a recognition of
               differing views about "what text really is", and hence a rethinking of some aspects
               of TEI praxis. We report on work carried out in the context of the COST Action
               CA16204 "Distant Reading", in particular on the TEI-conformant schemas developed for
               one of the Action's principal deliverables: the European Literary Text Collection
               (ELTeC). </p>
            <p>The ELTeC will contain comparable corpora for each of a dozen European languages,
               each being a balanced sample of 100 novels from the 19th century, together with
               metadata situating them in their contexts of production and of reception. We hope
               that it will become a reliable basis for comparative work in data-driven textual
               analytics, enabling researchers to go beyond a simple "bag of words" approach, while
               respecting views of "what text really is" currently dominant in such fields as
               statistically-derived authorship attribution, topic modelling, character network
               analysis, and stylistic analysis in general. </p>
            <p>The focus of the ELTeC encoding scheme is not to represent texts in all their
               original complexity, nor to duplicate the work of scholarly editors. Instead, we aim
               to facilitate a richer and better-informed distant reading than a transcription of
               lexical content alone would permit. Where the TEI permits diversity, we enforce
               consistency, by defining encodings which permit only a specific and quite small set
               of textual features, both structural and lexical. We also define a single
               TEI-conformant way of representing the results of textual analyses such as named
               entity recognition or morphological parsing, and a specific set of metadata features.
               These constraints are expressed by a master TEI ODD, from which we derive three
               different schemas by ODD-chaining, each associated with appropriate documentation.
            </p>
         </div>
         <div type="authors">
            <p>Lou Burnard is an independent consultant in TEI XML. He was for many years Associate
               Director of Oxford University Computing Services, and was one of the original editors
               of the TEI. </p>
            <p>Christof Sch√∂ch is Professor of Digital Humanities at the University of Trier,
               Germany, and Co-Director of the Trier Center for Digital Humanities (TCDH). He chairs
               the COST Action "Distant Reading for European Literary History" (CA16204).
               <!--Find out
               more at http://www.christof-schoech.de/en . --></p>
            <p>Carolin Odebrecht (Humboldt-Universit√§t zu Berlin)) is a corpus linguist. Her
               research fields are modelling, creating, archiving of historical corpora and corpus
               metadata. </p>
         </div>
      </front>
      <body>
         <div>
            <head>Introduction</head>
            <p>Comity is a term from theology or political studies, where it is used to describe the
               formal recognition by different religions, nation states, or cultures that other such
               entities have as much right to existence as themselves. In applied linguistics, the
               term has also been used by such writers as Widdowson or Aston seeking to demonstrate
               how the establishment of comity can facilitate successful inter-cultural
               communication, even in the absence of linguistic competence. We appropriate the term
               here in this latter sense, as a means of re-asserting the inter-disciplinary roots of
               the TEI. </p>
            <p>Recent histories of the TEI (e.g. Gavin, 2019) have a tendency to under-emphasize the
               multiplicity of disciplines gathered at its birth, preferring to focus on those
               disciplines which can be plausibly framed as prefiguring our current configuration of
               the "digital humanities" in some way. Yet the Poughkeepsie conference and the process
               of designing the Guidelines which followed alike were kickstarted by input from
               corpus linguists and computer scientists as well as traditional philologically-minded
               editors and source-driven historians. The TEI belongs to a multiplicity of research
               communities, dating as it does from a period when computational linguists and
               traditional philogists alike were beginning to wake up to the implications of the
               advent of massive amounts of digital text for their disciplines. The steering
               committee which oversaw its development and the TEI editors alike conscientiously
               attempted to ensure that the Guidelines should reflect a view of text which was
               generally shared and generic, rather than specific to any discipline or to any
               particular usage model. </p>
            <p>The TEI's radical proposition that there was such a thing as a single abstract model
               of textual components, which might usefully be considered independently of its
               expression in a particular source or output, or its use in any particular discipline,
               was necessarily at odds with at least two prevailing orthodoxies: on the one hand,
               the view that a text is no less and no more than the physical documents which
               instantiate it, and can be adequately described and represented by its salient visual
               properties alone; on the other hand, the view that a text is solely a linguistic
               phenomenon, comprising a bag of words, the statistical properties of which are
               adequate to describe it. But the TEI tried very hard to prefer comity over conflict,
               not opnly in its organization, which brought together an extraordinarily heterogenous
               group of experts, but also in its chief outputs: a set of encoding Guidelines which
               while supporting specialization did not require any particular specialisation to
               prevail. </p>
            <p>Old orthodoxies do not die easily, and it is interesting to hear some of the same arguments
               played out in different language in the different context of today's DH theorizers.
               But in our present paper, we simply want to explore the extent to which the TEI's
               model of text can be adapted to conform to the model of text characterising such
               fields as stylometry, stylistics, textual analytics, or (to use the current term)
               "distant reading". We hope also to explore the claim that by so doing we may
               facilitate the enrichment of that model, and thus facilitate more sophisticated
               research into textual phenomena across different corpora. </p>
         </div>
         <div>
            <head>COST Action 162014</head>
            <p>The context for this work is the COST Action CA16204 ‚ÄúDistant Reading for European
               Literary History‚Äù "Distant Reading" a principal deliverable of which will be the
               European Literary Text Collection (ELTeC). This is a set of comparable corpora for
               each of a dozen European languages, each being a balanced sample of 100 novels from
               the 19th century, together with metadata situating them in their contexts of
               production and of reception. It is hoped that the ELTeC will become a reliable basis
               for comparative work in cross-linguistic data-driven textual analytics by providing
               an accessible benchmark for a particular written genre of considerable importance
               across Europe during the period between 1840 and 1920. </p>
            <p>Two significant decisions made early on in the planning of the COST Action underlie
               the work reported here. Firstly, it was agreed that the ELTeC should be delivered in
               a TEI-encoded format, using a schema developed specifically for the project.
               Secondly, the design of that encoding scheme, in particular the textual features it
               makes explicit by means of markup, should be defined as far as possible by the needs
               of the distant reading research community, rather than any pre-existing notion of
               textual ontology, to the extent that the needs of that community could be determined.
               The target audience envisaged includes experts in computational stylistics, in corpus
               linguistics, and in traditional literary studies as well as more general digital
               humanists, but is probably best characterized as having major enthusiasm and
               expertise in the application of statistical methods to literary and linguistic
               analysis, and only minor interest in the kinds of textual features most TEI projects
               have tended to focus on. </p>
            <p>The work of the Action is carried out in four Working Groups (WGs), whose activities
               are subject to endorsement and acceptance by a Management Committee, composed of two
               national representatives from each of the 22 countries currently participating in the
               Action. The Working Group heads are also members of the smaller "core" group
               responsible for day to day management of the Action. WG1 "Scholarly Resources" is
               responsible for the work described in this paper; WG2 "Methods and Tools" is
               concerned with text analytic techniques and tools; WG3 "Literary Theory and History"
               is concerned with applications and implications of those methods and for literary
               theory ; WG4 "Dissemination" is responsible for outreach and communication. More
               information about the activities of the Action is available from its website at
               http://https://www.distant-reading.net. </p>
            <p>The design and construction of the ELTeC is the responsibility of WG1, as noted
               above. Initially, this work was split into three distinct tasks: First, defining
               selection criteria (corpus design); second, developing basic encoding methods (both
               for data and for metadata); and third, defining a suitable workflow for preparation
               of the corpus. Working papers on each of these topics plus a fourth on theoretical
               issues of sampling and balance were prepared for discussion and approval by the 30
               members of WG1. These documents are available from the Working Group's website at
               distantreading.github.io, and their proposals were ratified by the Management
               Committee after discussion at two meetings during 2018. </p>
         </div>
         <div>
            <head>The ELTeC Encoding Scheme/s</head>
            <p>The encoding requirements for ELTeC were perceived by WG1 to be somewhat different
               from those of many other TEI projects. Distant Reading methods cover a wide range of
               computational methods for literary text analysis, such as authorship attribution,
               topic modelling, character network analysis, or stylistic analysis but they are
               rarely concerned with editorial matters such as textual variation, the establishment
               of an authoritative text, or production of print or online versions of a text.
               Consequently, the ELTeC encoding scheme was deliberately not intended to represent
               texts in all their original complexity of structure or appearance, but rather to make
               it as simple as possible to access the words of which texts are composed in an
               informed and predictable way. The goal was neither to duplicate the work of scholarly
               editors nor to produce (yet another) digital edition of a specific source document.
               Rather, the encoding scheme was designed in such a way as to ensure that ELTeC texts
               could be processed by simple minded (but XML-aware) systems primarily concerned with
               lexis and to make life easier for the developers of such systems.</p>
            <p>An important principle following from this latter goal is that ELTeC markup should
               offer the encoder very little choice, and the software developer very few
               disagreeable surprises: the number of tags available is greatly reduced, and their
               application is relatively constrained. This facilitates processing greatly because
               access to each part of the XML tree can be provided in a uniform and consistent way
               across multiple ELTeC corpora.</p>
            <p>By default, the TEI provides a very rich vocabulary, and many subtly different ways
               of doing more or less the same thing. TEI encoders have frequently taken full
               advantage of that to produce texts which vary enormously, both in the subset of XML
               tags used and in the range of attribute values associated with them. It is tempting,
               but entirely mistaken, to assume that the "TEI conformant" deliverables from project
               A will necessarily be marked up in the same way as the "TEI conformant" deliverables
               from project B [see project Monk for evidence]. On the contrary, all that "TEI
               conformance" guarantees is that the intended semantics of the markup used by the two
               projects should be recoverable by reference to a published standard, and are not
               entirely ad hoc or sui generis. (This may not seem much of an advance, though it is). </p>
            <p>Following this "no surprises" principle, the simplest ELTeC schema (the
                  <soCalled>level zero </soCalled> schema) provides the bare minimum of tags needed
               to mark up the typical structure and content of a nineteenth century novel. All
               preliminary matter other than the titlepage and any authorial preface or introduction
               is discarded; the remainder is marked as a <gi>div</gi> of <att>type</att>
               <val>titlepage</val> or <val>liminal</val>, within a <gi>front</gi> element. Within
               the <gi>body</gi> of a text, the <gi>div</gi> element is also used to make explicit
               its structural organization, with <att>type</att> attribute values <val>part</val>,
                  <val>chapter</val>, or <val>letter</val> only<note place="foot">An exception is
                  made for epistolary novels which contain only the representation of a sequence of
                  letters, with no other significant content: these may be marked as <tag>div
                     type="letter"</tag></note>. For our purposes, a <q>chapter</q> is the smallest
               subsection of a novel within which paragraphs of text appear. Further subdivisions
               within a chapter (often indicated conventionally by elipses, dashes, stars etc.) are
               marked using the <gi>milestone</gi> element; larger groupings of <gi>div</gi>
               elements are all considered to be of type <val>part</val>, whatever their hierarchic
               level. Headings, at whatever level, are marked using the <gi>head</gi> element when
               appearing at the start of a <gi>div</gi>, and the <gi>trailer</gi> element when
               appearing at the end. Within the <gi>div</gi> element, only a very limited number of
               elements is permitted: specifically, in addition to those already mentioned,
                  <gi>p</gi> or <gi>l</gi> (verse line) and within these elements we find either
               plain text, <gi>hi</gi> (highlighted), <gi>pb</gi> (page break) or <gi>milestone</gi>
               elements. After some debate, the Action's Management Committee agreed that it would
               be practical to require only this tiny subset of the TEI for all ELTeC texts. </p>
            <p> It should be noted that the texts included in an ELTeC corpus may come from
               different kinds of source. For some language collections, no digital texts of any
               kind exist: the encoder must start from page images, put them through OCR, and
               introduce ELTeC markup from scratch. For others, existing digital texts may already
               be available: the encoder must research the format used and find a way of converting
               it to ELTeC. In some cases, a TEI version may already exist; in others a project
               Gutenberg HTML version; in yet others the text may be stored in a database of some
               kind. Whichever is the case, if it is possible to retain distinctions which the ELTeC
               scheme permits, this is clearly desirable; perhaps less obviously, it is also
               necessary to remove distinctions made by the original format which the ELTeC scheme
               does not permit. This diversity of source material was one motivation for permitting
               multiple encoding levels in the ELTeC scheme: at level zero, only the bare minimum of
               tags listed above is required or permitted, while at level 1 a slightly richer (but
               still minimalist) encoding is also defined. At level 2, further tags again are
               introduced to support linguistic processing of various kinds, as discussed further
               below. Down-conversion from a higher to a lower level is always automatically
               possible, but up conversion from a lower to a higher level generally requires human
               intervention or additional processing. </p>
            <p>At level 1, the following additional distinctions may be made in an encoding: <list>
                  <item>the <gi>label</gi> element may be used for heading-like titles appearing in
                     the middle of a division; </item>
                  <item>the <gi>quote</gi> element may be used to distinguish passages such as
                     quotations, epigraphs, stretches of verse, letters etc. which seem to
                        <q>float</q> within the running text; </item>
                  <item>the <gi>corr</gi> element may also be used to indicate a passage (typically
                     a word or phrase) which has been is clearly erroneous in the original and which
                     has been editorially corrected; </item>
                  <item>the elements <gi>foreign</gi>, <gi>emph</gi>, or <gi>title</gi> are
                     available and should be used in preference to <gi>hi</gi> for passages rendered
                     in a different font or otherwise made salient in the source where an encoder
                     can do so with confidence; </item>
                  <item>the element <gi>gap</gi> may be used to indicate where some component of a
                     source (typically an illustration) has been left out of the encoding; </item>
                  <item>the elements <gi>note</gi> and <gi>ref</gi> may be used to capture the
                     location and content of authorially supplied footnotes or end-notes</item>
               </list></p>
            <p>For those already familiar with the TEI, this list of elements may seem distressingly
               small. It lacks entirely some elements which every TEI introductory course regards as
               indispensable (no <gi>list</gi> or <gi>item</gi>; no <gi>choice</gi> or
               <gi>abbr</gi>; no <gi>name</gi> or <gi>date</gi>...) and tolerates some practices
               bordering on tag abuse. For example, all the components of a title page are marked as
                  <gi>p</gi> since no specialised elements are available. In the absence of
               specialised but culture-specific features (for example, publisher name, imprint,
               imprimatur, etc.) the encoding identifies more fundamental textual features common to
               every kind of text. We believe that the set of concepts it supports overlaps well
               with the set of textual features which almost any existing digital transcription will
               seek to preserve in some form or another. This may explain both why the majority of
               the texts so far collected in the ELTeC have been encoded at level 1 rather than
               level0, and also the speed with which the collection is growing.</p>
            <!-- example? -->
            <p>ELTeC level 1 is intended to facilitate a richer and better-informed distant reading
               than a transcription of lexical content alone would permit. ELTeC level 2 is intended
               to provide a consistent and TEI-conformant way of representing the results of that
               distant reading. Enrichment of each lexical token to indicate such features as its
               morpho-syntactic category (POS) or lemma, and identification of tokens which refer to
               named entities is well within the scope of existing text processing techniques: the
               challenge is that both the input and the output formats typically used by such tools
               are rarely XML-based, and seem superficially to have a model of text quite different
               from that of the "ordered hierarchy of content objects" in terms of which the TEI
               community traditionally operates. In that model, a text is composed of a sequence of
               tokens, mostly corresponding with orthographically-defined words, though there is
               some support for the notion of "muli-word" tokens. Each token has a number of
               properties, which might include such attributes as its part of speech, its lemma, or
               its position in the sequence of tokens making up the document, but also information
               which in an XML model would be properties of some higher level construct such as its
               status as dialogue, quoted matter, emphasis, etc. </p>
            <p>At ELTeC level2, all existing elements are retained, but two new elements <gi>s</gi>
               and <gi>w</gi> are introduced. Individual tokens are marked using the <gi>w</gi>
               elements, and decorated with one or more of the linguistic attributes <att>pos</att>,
                  <att>lemma</att>, and <att>join</att>. The <gi>s</gi> (segment) element is used to
               provide an end-to-end tessellating segmentation of the whole sequence of <gi>w</gi>
               elements, providing a convenient extension of the existing text-body-div hierarchy
               within which tokens are located. The elements <gi>p</gi>, <gi>head</gi>, and
                  <gi>l</gi> (which contain just text at levels 0 and 1) at level 2 can contain a
               sequence of <gi>s</gi> elements. At any level, empty elements such as <gi>gap</gi>,
                  <gi>milestone</gi>, <gi>pb</gi> or <gi>ref</gi> can appear within text, but these
               are disregarded when segmentation is carried out. Each <gi>s</gi> element can contain
               a sequence of <gi>w</gi> elements, either directly, or wrapped in one of the
               sub-paragraph elements <gi>corr</gi>, <gi>emph</gi>, <gi>foreign</gi>, <gi>hi</gi>,
                  <gi>label</gi>, <gi>title</gi>. To this list we might add the element <gi>rs</gi>
               (referring string), provided by the TEI for the encoding of any form of entity name,
               such as a Named Entity Recognition procedure might produce. </p>
            <p> This approach implies that <gi>w</gi> elements may appear at two levels in the
               hierarchy which may upset some software; it also implies that <gi>w</gi> elements
               must be properly contained within one of these elements. If either issue proves to be
               a major stumbling block, an alternative would be to remove the tags demarcating these
               sub-paragraph elements, indicating their status by additional attribute values on the
                  <gi>w</gi> elements they contain. </p>
            <p>This TEI XML format is equally applicable to the production of training data for
               applications using machine learning techniques as it is to the outputs of such
               systems. Such machine learning applications typically operate on text content in a
               tabular format only ; however XSLT filters which transform (or generate) the XML
               markup discussed here from such tabular formats without loss of information are
               easily imagined. At the time of writing, however, Working Group 2 has yet to put this
               proposed architecture to the test. </p>
            <!-- examples -->
            <!--
            <s><w>Here</w><w join="left">'s</w><w>a</w><emph><w>really</w><w>silly</w></emph><w>example</w><w join="left">.</w></s>
           might become without loss of information
              <s><w>Here</w><w join="left">'s</w><w>a</w><w context="emph">really</w><w context="emph">silly</w><w>example</w><w join="left">.</w></s>
       -->
         </div>
         <div>
            <head>ELTeC metadata and corpus design</head>
            <p>Like every other TEI document, every ELTeC text has a TEI Header, though its
               organization and content alike are constrained much more tightly than is common TEI
               praxis, for the reasons already mentioned. The structure of an ELTeC Header is the
               same no matter what level of encoding applies to the text. It provides minimal
               bibliographic information about the encoded text and its source, sufficient to
               identify the text and its author, in a fixed and consistent format. It is assumed
               that if more detailed bibliographic information is required, for example about the
               author or work encoded, this is better obtained from standard authority files; to
               that end a VIAF code may be associated with them. </p>
            <!-- example here? -->
            <p>As noted above, ELTeC texts may be derived from many sources, each of which should be
               documented correctly in the header's <gi>sourceDesc</gi> element. After some debate,
               a common set of practices has been identified to distinguish (for example) ELTeC
               texts derived directly from a print source from those derived from a digital source,
               itself derived from a known print source, and to provide information about each
               source. In the following example, the source of the ELTeC version is a pre-existing
               digital edition provided by the Biblioth√®que nationale de France at the address
               specified (ARK -- Archival Resource Key -- is the acronym used by the BNF for the
               permanent identifiers it provides for its digital collections). The sopurce
               description also provides information about the first print edition of the work
               concerned.
               <egXML xmlns="http://www.tei-c.org/ns/Examples">
                  <sourceDesc>
                     <bibl type="digitalSource">
                        <title>Tatiana Le√Ølof: roman parisien (√©dition numeris√©e)</title>
                        <publisher> gallica.bnf.fr / Biblioth√®que nationale de France </publisher>
                        <idno type="ARK">12148/bpt6k931128v</idno>
                     </bibl>
                     <bibl type="firstEdition">
                        <title>Tatiana Le√Ølof , roman parisien, par √âdouard Rod</title>
                        <publisher>E. Plon, Nourrit et Cie</publisher>
                        <pubPlace>Paris</pubPlace>
                        <date>1886</date>
                     </bibl>
                  </sourceDesc>
               </egXML>
                In most cases, the ELTeC text will correspond with the first edition of a work in
               book form; but even where this is not the case, or where information about the
               precise source used is not available, minimal information about that first edition
               should also be provided in order to place the work in its original temporal context. </p>
            <p>As with other TEI conformant documents, beside the mandatory file description, the
               TEI Header of every ELTeC text contains a publication statement which specifies its
               licensing conditions (all ELTeC documents are licensed CC-BY); an encoding statement
               specifying the level of encoding used; and a revision description containing optional
               versioning information. The TEI Header is also used to provide metadata describing
               its associated text in a standardized form; this is held in the <gi>profileDesc</gi>
               element which must specify the languages used by the text, and may optionally include
               a <gi>textClass</gi> element containing any keywords considered useful to describe
               the text. It must also contain a <gi>textDesc</gi> element which documents the text's
               status with respect to selection criteria discussed below. </p>
            <p>One of the knottier problems or (to be positive) more distinctive features of an
               ELTeC language collection is that it is not intended to be an ad hoc accidentally
               constructed corpus but a designed one. Its composition is determined not by the
               happenstance of whatever we can get our hands on, but is instead defensible, at least
               in theory, as a principled and representative sample.</p>
            <p>The big question is, of course, representative of <hi>what</hi>.</p>
            <p> It would be nice to say that it represents the production of novels in a specific
               language in 19th century Europe. WG1 has working definitions for both "novels" and
               "Europe" which we do not discuss further here, though both are clearly problematic
               terms. It is hoped that the ELTeC will provide data for an empirical discussion of
               such terms, feeding into the work of WG3 on literary theory and terminology. </p>
            <p> But we cannot make that claim without any data about the population we are claiming
               to represent -- which is hard to come by for most if not all languages. We know about
               the novels which we know about, which tend to be the ones that national libraries or
               equivalent cultural heritage institutions have chosen to preserve, which publishers
               over time have been able to sell, and which lecturers in English have chosen to
               teach. More ephemeral titles may have been collected (for example by a copyright
               library); but equally well may have been discarded or even suppressed as unworthy of
               inclusion in the national patrimony. Titles and authors alike can go in and out of
               fashion. But how can we express opinions about changes in the nature of the published
               novel if the sample on which we base those opinions is wildly different in
               composition from the actual population? If our data leads us to assert that novels in
               a given language are never written by women, or are never of fewer than 100,000 words
               is this simply because no female authors happen to have been preserved, or because
               short novels were routinely discarded from the collection? Or, on the other hand,
               does this actually indicate something fundamental, a characteristic of the population
               we are investigating? This matters particularly for ELTeC, one of the goals of which
               is precisely to facilitate cross-language comparisons.</p>
            <p>This problem of representativeness is of course one which every corpus linguist has
               to face, and discussions of its implications are easy to find in the literature <note
                  place="foot">Notable examples include Biber, Douglas (1993). ‚ÄúRepresentativeness
                  in Corpus Design‚Äù. In: Literary and Linguistic Computing (8), pp. 243‚Äì257. Bode,
                  Katherine (2018). A World of Fiction - Digital Collections and the Future of
                  Literary History. eng. University of Michigan Press. L√ºdeling, Anke (2011).
                  ‚ÄúCorpora in Linguistics. Sampling and Annotation‚Äù. In: Going Digital. Evolutionary
                  and Revolutionary Aspects of Digitization. Ed. by Karl Grandin. Vol. 147. Nobel
                  Symposium 147. New York:</note></p>
            <p>Our approach is to sidestep the impossibility of representing an unknown (and
               sometimes unknowable) population by attempting instead to represent the range of
               variation possible in the values for some specific categories applicable to all
               members of that population. To take a trivial example, every novel can be
               characterised as short, medium, or long; there is no possible fourth value for this
               category unless we decide to revise our definition of length (elastic? unknown?
               instantaneous?). So, as a working hypothesis, we might say that a corpus in which
               roughly a third of the titles are short, a third are long, and a third are medium
               will represent the variation possible for this category. If we apply this principle
               uniformly across all our corpora, we can reliably investigate (for example) cross
               language variation in some other observable phenomenon (say fondness for embedded
               sentences) with respect to length. But note that we have made absolutely no claim
               about whether novel length in the underlying population is also divided in this way. </p>
            <p>The decade in which a novel first appears in book form is a similarly objectively
               characteristic, which in principle we can determine for every member of the
               population. We can also classify every title according to the actual sex of their
               author (with values such as female, male, mixed, unknown). And we can likewise
               classify a title in terms of its "staying power" or persistence by looking at the
               number of times it has been reprinted since its first appearance. We suggest that
               texts which have been frequently reprinted over a long period may reasonably be
               considered "canonical" in some sense of that vexed term. The goal of our corpus
               balancing exercise is to ensure more or less equal time for each possible value for
               each of these four categories -- size, decade, authorSex, and canonicity. </p>
            <p>Ideally, each corpus should have equivalent numbers not just for each value, but for
               each combination of values: so, for example, looking at the third of all titles which
               are characterised as "short", there should be roughly equal numbers for each decade
               of first appearance, roughly equal numbers by male and female authors, and so on.
               This may however be a council of perfection. It is already apparent that for some
               languages, it is vcery difficult to find any texts at all within some time periods,
               or by female authors. Similarly, our definition of "short" (10-50 thousand words),
               "medium" (50-100 thousand words) and "long" (over 100 thousand words) though
               objective and easy to validate, assumes that there will be enough novels of a given
               length in the underlying population for us to extract a balanced sample; but in some
               languages it may be that the distribution of lengths across the population is
               entirely different. We cannot tell whether (for example) the absence of any "long"
               novels at all in Czech, Serbian or Norwegian is characteristic of those languages, or
               an artefact of the selection process so far. Another difficulty is that our corpus
               design deliberately seeks to include some forgotten or marginal works along with
               well-known canonical texts: this is relatively easy for traditions such as English,
               French, or German where copyright laws have led to the maintenance and documentation
               of large national collections, but less so for other less well documented languages.
               (For some initial data, see the summary page at <ref
                  target="http://distantreading.github.io/ELTeC/"
                  >https://distantreading.github.io/ELTeC/</ref>)</p>
            <p>To encode these balance criteria in the TEI Header in as direct and accessible a
               manner as possible, we have chosen to repurpose the little-used <gi>textDesc</gi>
               element, originally provided by the TEI as a wrapper for a set of so-called
               situational parameters proposed by corpus linguists as a way of objectively
               characterizing linguistic production [ref]. In our case, we replace the TEI's
               vocabulary for these parameters with a vocabulary representing our four criteria,
               expressed as new non-TEI elements in the ELTeC namespace. These elements
                  (<gi>eltec:sex</gi>, <gi>eltec:size</gi>, <gi>eltec:canonicity</gi>, and
                  <gi>eltec:timeSlot</gi>) are required by the ELTeC schema and have an attribute
                  <att>key</att> which supplies a coded value for the criterion concerned taken from
               a predefined closed list. So, for example, a long (over 100k words) novel by a female
               author first published between 1881 and 1900, but only infrequently reprinted
               thereafter might have a text description like the following:
               <egXML xmlns="http://www.tei-c.org/ns/Examples">
                  <textDesc xmlns:eltec="http://distant-reading.net/ns">
                     <eltec:authorGender key="F"/>
                     <eltec:canonicity key="low"/>
                     <eltec:size key="long"/>
                     <eltec:timeSlot key="T3"/>
                  </textDesc>
               </egXML>
            </p>
            <p>The intention is to use this information can be used to select subcorpora from the
               collection as a whole, and </p>
         </div>
         <div>
            <head>Chaining ODDs</head>
            <p>The TEI's ODD (One Document Does it all) system is widely used as a means of
               customizing the TEI and documenting the customization in a standard way [Rahtz et
               al]. When only a single ODD customization is used across a project, there is a
               natural tendency to produce broadly permissive schemas, to allow for the inevitable
               variation of requirements when material of different kinds are to be processed in an
               integrated collection. But this prevents the encoder from taking full advantage of
               the ability of an XML schema to check that particular documents conform to predefined
               rules, unless they are willing greatly to increase the complexity of the workflow. A
               better approach, pioneered by the Deutsche Textarchiv [ref], has been the use of a
               technique known as ODD chaining [ODD Chaining Tuto]. Here, a project first defines a
               base ODD which selects all the TEI components considered to be useful anywhere and
               then uses this as the basis for smaller, more constraining, ODDs which select from
               the base only the components (or other rules) specific to a subset of the project's
               documentary universe. For example, an archive may have identified both a common set
               of metadata it wishes to document across all of its holdings and also different
               metadata requirements for print and manuscript sources. Simply defining two different
               ODDs, one for print and one for manuscript, when many other components apply to
               either kind of source opens the door to redundant duplication and the risk of
               inconsistency. The ODD chaining approach requires definition of a base ODD which
               contains the union of the components needed for these two different ODDs, constructed
               as an appropriate selection from the full range of TEI componentsen. The ODDs for
               print and manuscript are then defined as further specialisations or customizations of
               the base, ensuring thereby that the common components are used in a consistent
               manner. </p>
            <p>In the ELTeC project, we begin by defining an ODD which selects from the TEI all the
               components used by any ELTeC schema at any level. This ODD also contains
               documentation and specifies usage constraints applicable across every schema. This
               "base" ODD is then processed using the TEI standard <ident>odd2odd</ident> stylesheet
               to produce a standalone set of TEI specifications which we call
                  <ident>eltec-library</ident>. Three different ODDs, eltec-0, eltec-1, and eltec-2
               then derive specific schemas and documentation for each of the three ELTeC levels,
               using this library of specifications as a base rather than the whole of the TEI. This
               enables us to customize the TEI across the whole project. As with other ODDs, we are
               thus able to produce documentation and formal schemas which reflect exactly the scope
               of each encoding level.</p>
            <p> The ODD sources and their outputs are maintained on GitHub and will also be
               published on Zenodo along with the ELTeC texts. [ref?] </p>
         </div>
         <div>
            <head>State of play and future work</head>
            <p>The ELTeC is still very much a work in progress, and hence we cannot report with any
               confidence that our design goals have been achieved. The first Zenodo release of the
               corpus is due in September 2019, and we expect several future releases before the
               target of 100 texts per language is achieved. As noted above, testing of the proposed
               ELTeC level 2 encoding is needed, and may necessitate some changes in the existing
               markup scheme.
               <!--  ‚Ä¢ develop a composition plot including reprint criteria
               ‚Ä¢ first corpus release in Sep 2019
               ‚Ä¢ Creating novels in level 2 in cooperation with WG2
               Improve documentation on GitHub and Distant reading homepage--></p>
         </div>
      </body>
      <back>
         <listBibl>
            <head>References</head>
            <bibl>
               <author>Gavin, Michael</author>
            </bibl>
            <bibl>Caton, Paul (2013). ‚ÄúOn the term text in digital humanities‚Äù. In: Literary and
               Linguistic Computing 28.2, pp. 209‚Äì220. </bibl>
            <bibl> De Rose, Steven J., David G. Durand, Elli Mylonas, and Allen H. Renear (2002).
               ‚ÄúWhat is Text, Really?‚Äù In: Journal of Computing in Higher Education I(2), pp. 3‚Äì26.
               (Visited on 04/05/2016). </bibl>
            <bibl>van Zundert, Joris and Tara L. Andrews (2017). ‚ÄúQu‚Äôest-ce qu‚Äôun texte num√©rique? A
               new rationale for the digital representation of text‚Äù. In: Digital Scholarship in the
               Humanities 32, pp. 78‚Äì88. </bibl>
            <bibl>Biber, Douglas (1993). ‚ÄúRepresentativeness in Corpus Design‚Äù. In: Literary and
               Linguistic Computing (8), pp. 243‚Äì257.</bibl>
            <bibl>Bode, Katherine (2018). A World of Fiction - Digital Collections and the Future of
               Literary History. eng. University of Michigan Press. </bibl>
            <bibl>L√ºdeling, Anke (2011). ‚ÄúCorpora in Linguistics. Sampling and Annotation‚Äù. In:
               Going Digital. Evolutionary and Revolutionary Aspects of Digitization. Ed. by Karl
               Grandin. Vol. 147. Nobel Symposium 147. New York:</bibl>
         </listBibl>
      </back>
   </text>
</TEI>
